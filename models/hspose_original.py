
'''Original HS-Pose Implementation for Comparison
=============================================

This module contains the original HS-Pose implementation from the CVPR 2023 paper:
"HS-Pose: Hybrid Scope Feature Extraction for Category-level Object Pose Estimation"
'''

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class OriginalHybridScopeFeatureExtractor(nn.Module):
    """
    Original Hybrid Scope Feature Extractor from HS-Pose paper
    
    This implements the core feature extraction mechanism exactly as described
    in the original paper, without vehicle-specific modifications.
    """
    
    def __init__(self, input_channels=3, num_categories=6):
        """
        Initialize the original HS-Pose feature extractor
        
        Args:
            input_channels (int): Number of input channels (3 for RGB)
            num_categories (int): Number of object categories (6 in original)
        """
        super(OriginalHybridScopeFeatureExtractor, self).__init__()
        
        self.num_categories = num_categories
        
        # Local scope pathway - captures fine-grained details
        # Uses smaller receptive fields for precise localization
        self.local_scope = nn.Sequential(
            # First local convolution block
            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            # Second local convolution block
            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            # Third local convolution block
            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
        )
        
        # Global scope pathway - captures global context
        # Uses larger receptive fields for scene understanding
        self.global_scope = nn.Sequential(
            # First global convolution block (large kernel)
            nn.Conv2d(input_channels, 32, kernel_size=7, padding=3, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            # Second global convolution block
            nn.Conv2d(32, 64, kernel_size=5, padding=2, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            # Third global convolution block
            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
        )
        
        # Feature fusion module
        # Combines local and global features
        self.feature_fusion = nn.Sequential(
            nn.Conv2d(256 + 128, 512, kernel_size=1, bias=False),  # 384 -> 512
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
        )
        
        # Original pose estimation head
        # Estimates 6D pose (translation + rotation)
        self.pose_head = nn.Sequential(
            nn.Conv2d(512, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(256, 128, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            # Global average pooling to get fixed-size output
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            
            # Final pose regression layers
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(64, 6),  # 6D pose: [tx, ty, tz, rx, ry, rz]
        )
        
        # Category classification head
        # Classifies object category
        self.category_head = nn.Sequential(
            nn.Conv2d(512, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(128, num_categories),
        )
        
"""
Original HS-Pose Implementation for Comparison
=============================================

This module contains the original HS-Pose implementation from the CVPR 2023 paper:
"HS-Pose: Hybrid Scope Feature Extraction for Category-level Object Pose Estimation"

Authors: Linfang Zheng, Chen Wang, Yinghan Sun, Esha Dasgupta, 
         Hua Chen, AleÅ¡ Leonardis, Wei Zhang, Hyung Jin Chang
Paper: https://arxiv.org/abs/2303.15743
Original Code: https://github.com/Lynne-Zheng-Linfang/HS-Pose

This implementation adapts the original HS-Pose for comparison with our vehicle-specific version.
The original HS-Pose was designed for general category-level object pose estimation on NOCS dataset.

Key Features of Original HS-Pose:
- Hybrid scope feature extraction (local + global pathways)
- Multi-scale feature fusion
- Category-level pose estimation for 6 object categories
- NOCS coordinate prediction
- Size and pose estimation
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os

class OriginalHybridScopeFeatureExtractor(nn.Module):
    """
    Original Hybrid Scope Feature Extractor from HS-Pose paper
    
    This implements the core feature extraction mechanism exactly as described
    in the original paper, adapted from the official implementation.
    """
    
    def __init__(self, input_channels=3, num_categories=6, num_points=1024):
        """
        Initialize the original HS-Pose feature extractor
        
        Args:
            input_channels (int): Number of input channels (3 for RGB)
            num_categories (int): Number of object categories (6 in original NOCS)
            num_points (int): Number of points for NOCS prediction
        """
        super(OriginalHybridScopeFeatureExtractor, self).__init__()
        
        self.num_categories = num_categories
        self.num_points = num_points
        
        # ResNet-34 backbone (as used in original HS-Pose)
        self.backbone = self._make_resnet34_backbone(input_channels)
        
        # Local scope pathway - captures fine-grained details
        self.local_scope = nn.Sequential(
            # Local feature extraction with small kernels
            nn.Conv2d(512, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
        )
        
        # Global scope pathway - captures global context
        self.global_scope = nn.Sequential(
            # Global feature extraction with larger kernels
            nn.Conv2d(512, 128, kernel_size=7, padding=3, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(128, 128, kernel_size=5, padding=2, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
        )
        
        # Feature fusion module (original HS-Pose approach)
        self.feature_fusion = nn.Sequential(
            nn.Conv2d(256 + 128, 512, kernel_size=1, bias=False),  # 384 -> 512
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
        )
        
        # NOCS coordinate regression head (original HS-Pose feature)
        self.nocs_head = nn.Sequential(
            nn.Conv2d(512, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(256, 128, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(128, 3, kernel_size=1),  # 3D NOCS coordinates
        )
        
        # Mask prediction head
        self.mask_head = nn.Sequential(
            nn.Conv2d(512, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(256, 128, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(128, 1, kernel_size=1),  # Binary mask
            nn.Sigmoid(),
        )
        
        # Category classification head
        self.category_head = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, num_categories),
        )
        
        # Size estimation head (original HS-Pose)
        self.size_head = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, 3),  # 3D size
        )
        
        # Pose estimation head (6D pose)
        self.pose_head = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, 6),  # 6D pose [tx, ty, tz, rx, ry, rz]
        )
        
        # Initialize weights
        self._initialize_weights()
    
    def _make_resnet34_backbone(self, input_channels):
        """Create ResNet-34 backbone as used in original HS-Pose"""
        # This is a simplified ResNet-34 implementation
        # In practice, you would load a pretrained ResNet-34 and modify the first layer
        
        layers = []
        
        # Initial convolution
        layers.append(nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False))
        layers.append(nn.BatchNorm2d(64))
        layers.append(nn.ReLU(inplace=True))
        layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
        
        # ResNet blocks (simplified)
        layers.extend([
            self._make_layer(64, 64, 3),
            self._make_layer(64, 128, 4, stride=2),
            self._make_layer(128, 256, 6, stride=2),
            self._make_layer(256, 512, 3, stride=2),
        ])
        
        return nn.Sequential(*layers)
    
    def _make_layer(self, inplanes, planes, blocks, stride=1):
        """Create ResNet layer"""
        layers = []
        
        # First block with potential stride
        layers.append(nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False))
        layers.append(nn.BatchNorm2d(planes))
        layers.append(nn.ReLU(inplace=True))
        
        # Remaining blocks
        for _ in range(1, blocks):
            layers.append(nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False))
            layers.append(nn.BatchNorm2d(planes))
            layers.append(nn.ReLU(inplace=True))
        
        return nn.Sequential(*layers)
    
    def _initialize_weights(self):
        """Initialize network weights following original HS-Pose methodology"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """
        Forward pass through the original HS-Pose network
        
        Args:
            x (torch.Tensor): Input tensor of shape (B, C, H, W)
                             where C=3 for RGB input
        
        Returns:
            dict: Dictionary containing all prediction outputs
        """
        # Extract features using ResNet backbone
        backbone_features = self.backbone(x)
        
        # Extract features using hybrid scope approach
        local_features = self.local_scope(backbone_features)      # Fine-grained features
        global_features = self.global_scope(backbone_features)    # Global context features
        
        # Fuse local and global features
        combined_features = torch.cat([local_features, global_features], dim=1)
        fused_features = self.feature_fusion(combined_features)
        
        # Multi-task predictions (original HS-Pose outputs)
        nocs = self.nocs_head(fused_features)           # NOCS coordinates
        mask = self.mask_head(fused_features)           # Object mask
        category = self.category_head(fused_features)   # Category classification
        size = self.size_head(fused_features)           # Size estimation
        pose = self.pose_head(fused_features)           # 6D pose
        
        return {
            'nocs': nocs,               # NOCS coordinate map (B, 3, H, W)
            'mask': mask,              # Object mask (B, 1, H, W)
            'category': category,       # Category logits (B, num_categories)
            'size': size,              # Size estimation (B, 3)
            'pose': pose,              # 6D pose (B, 6)
            'features': fused_features  # Feature representation (B, 512, H, W)
        }

class OriginalHSPose(nn.Module):
    """
    Complete Original HS-Pose Model
    
    This is the full HS-Pose model as described in the original paper,
    including all prediction heads and post-processing functionality.
    """
    
    def __init__(self, input_channels=3, num_categories=6, num_points=1024):
        """
        Initialize the complete HS-Pose model
        
        Args:
            input_channels (int): Number of input channels (3 for RGB)
            num_categories (int): Number of object categories
            num_points (int): Number of points for NOCS prediction
        """
        super(OriginalHSPose, self).__init__()
        
        self.num_categories = num_categories
        self.num_points = num_points
        self.feature_extractor = OriginalHybridScopeFeatureExtractor(
            input_channels, num_categories, num_points
        )
        
        # Original NOCS category names
        self.category_names = [
            'bottle', 'bowl', 'camera', 'can', 'laptop', 'mug'
        ]
        
        # NOCS normalization parameters (from original paper)
        self.nocs_scale = 1000.0  # Scale factor for NOCS coordinates
        
    def forward(self, x):
        """Forward pass through the complete model"""
        return self.feature_extractor(x)
    
    def predict_pose(self, image, category_hint=None):
        """
        Predict pose using the original HS-Pose methodology
        
        Args:
            image (torch.Tensor): Input image tensor
            category_hint (str, optional): Category hint (not used in original)
        
        Returns:
            dict: Prediction results in original HS-Pose format
        """
        self.eval()
        with torch.no_grad():
            # Preprocess input
            if isinstance(image, np.ndarray):
                image = torch.from_numpy(image).float()
                if len(image.shape) == 3:
                    image = image.unsqueeze(0)
            
            # Ensure correct device
            device = next(self.parameters()).device
            image = image.to(device)
            
            # Forward pass
            outputs = self(image)
            
            # Extract predictions
            nocs = outputs['nocs']  # NOCS coordinate map
            mask = outputs['mask']  # Object mask
            pose = outputs['pose'].squeeze()
            category_logits = outputs['category'].squeeze()
            size = outputs['size'].squeeze()
            
            # Get predicted category
            predicted_category = torch.argmax(category_logits).item()
            category_confidence = torch.softmax(category_logits, dim=0)[predicted_category].item()
            
            # Post-process NOCS coordinates
            nocs_normalized = torch.tanh(nocs)  # Normalize to [-1, 1]
            
            # Extract 6D pose components
            translation = pose[:3]
            rotation = pose[3:6]
            
            return {
                'nocs': nocs_normalized.cpu().numpy(),        # NOCS coordinate map
                'mask': mask.cpu().numpy(),                   # Object mask
                'pose': pose.cpu().numpy(),                   # 6D pose
                'translation': translation.cpu().numpy(),     # 3D translation
                'rotation': rotation.cpu().numpy(),           # 3D rotation
                'category': predicted_category,               # Category ID
                'category_name': self.category_names[predicted_category],
                'category_confidence': category_confidence,   # Classification confidence
                'size': size.cpu().numpy(),                   # Size estimation
                'method': 'original_hspose'                   # Method identifier
            }
    
    def extract_point_cloud_from_nocs(self, nocs_map, mask, num_points=1024):
        """
        Extract point cloud from NOCS coordinate map (original HS-Pose functionality)
        
        Args:
            nocs_map (torch.Tensor): NOCS coordinate map
            mask (torch.Tensor): Object mask
            num_points (int): Number of points to sample
        
        Returns:
            torch.Tensor: Point cloud in NOCS coordinates
        """
        # Get valid pixels (where mask > 0.5)
        valid_mask = mask > 0.5
        
        if valid_mask.sum() == 0:
            # Return empty point cloud if no valid pixels
            return torch.zeros(num_points, 3)
        
        # Extract NOCS coordinates for valid pixels
        valid_nocs = nocs_map[:, valid_mask.squeeze()]  # (3, N)
        
        # Sample points if necessary
        if valid_nocs.shape[1] > num_points:
            # Random sampling
            indices = torch.randperm(valid_nocs.shape[1])[:num_points]
            sampled_nocs = valid_nocs[:, indices]
        else:
            # Pad with zeros if not enough points
            sampled_nocs = torch.zeros(3, num_points)
            sampled_nocs[:, :valid_nocs.shape[1]] = valid_nocs
        
        return sampled_nocs.T  # (num_points, 3)

# Loss functions for original HS-Pose
class OriginalHSPoseLoss(nn.Module):
    """
    Original HS-Pose loss function implementation
    
    Combines multiple loss components as described in the original paper:
    - NOCS coordinate regression loss
    - Mask prediction loss
    - Pose regression loss (translation + rotation)
    - Category classification loss
    - Size regression loss
    """
    
    def __init__(self, nocs_weight=1.0, mask_weight=1.0, pose_weight=1.0, 
                 category_weight=0.5, size_weight=0.3):
        super(OriginalHSPoseLoss, self).__init__()
        
        self.nocs_weight = nocs_weight
        self.mask_weight = mask_weight
        self.pose_weight = pose_weight
        self.category_weight = category_weight
        self.size_weight = size_weight
        
        # Loss functions
        self.nocs_loss = nn.MSELoss()
        self.mask_loss = nn.BCELoss()
        self.pose_loss = nn.MSELoss()
        self.category_loss = nn.CrossEntropyLoss()
        self.size_loss = nn.MSELoss()
    
    def forward(self, predictions, targets):
        """
        Compute multi-task loss for original HS-Pose
        
        Args:
            predictions (dict): Model predictions
            targets (dict): Ground truth targets
        
        Returns:
            dict: Individual and total losses
        """
        # NOCS coordinate loss
        nocs_loss = self.nocs_loss(predictions['nocs'], targets['nocs'])
        
        # Mask prediction loss
        mask_loss = self.mask_loss(predictions['mask'], targets['mask'])
        
        # Pose loss (translation + rotation)
        pose_loss = self.pose_loss(predictions['pose'], targets['pose'])
        
        # Category classification loss
        category_loss = self.category_loss(predictions['category'], targets['category'])
        
        # Size regression loss
        size_loss = self.size_loss(predictions['size'], targets['size'])
        
        # Total weighted loss
        total_loss = (
            self.nocs_weight * nocs_loss +
            self.mask_weight * mask_loss +
            self.pose_weight * pose_loss +
            self.category_weight * category_loss +
            self.size_weight * size_loss
        )
        
        return {
            'total_loss': total_loss,
            'nocs_loss': nocs_loss,
            'mask_loss': mask_loss,
            'pose_loss': pose_loss,
            'category_loss': category_loss,
            'size_loss': size_loss
        }

# Utility functions for original HS-Pose compatibility
def load_original_hspose_model(checkpoint_path=None, device='cpu'):
    """
    Load a trained original HS-Pose model
    
    Args:
        checkpoint_path (str): Path to model checkpoint
        device (str): Device to load model on
    
    Returns:
        OriginalHSPose: Loaded model
    """
    model = OriginalHSPose()
    
    if checkpoint_path and os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        # Handle different checkpoint formats
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        elif 'state_dict' in checkpoint:
            model.load_state_dict(checkpoint['state_dict'])
        else:
            model.load_state_dict(checkpoint)
            
        print(f"Loaded original HS-Pose model from {checkpoint_path}")
    else:
        print("Initializing original HS-Pose with random weights")
    
    model.to(device)
    return model

def convert_vehicle_to_original_format(vehicle_prediction):
    """
    Convert vehicle pose prediction to original HS-Pose format
    
    This function helps compare results between our vehicle adaptation
    and the original HS-Pose implementation.
    
    Args:
        vehicle_prediction (dict): Vehicle pose prediction
    
    Returns:
        dict: Prediction in original HS-Pose format
    """
    # Map vehicle categories to original categories (for comparison)
    vehicle_to_original_category = {
        0: 2,  # car -> camera (similar complexity)
        1: 4,  # motorcycle -> laptop (similar size)
        2: 0   # bicycle -> bottle (simple structure)
    }
    
    original_category = vehicle_to_original_category.get(
        vehicle_prediction.get('predicted_category', 0), 0
    )
    
    return {
        'nocs': np.zeros((3, 64, 64)),  # Placeholder NOCS map
        'mask': np.ones((1, 64, 64)),   # Placeholder mask
        'pose': np.concatenate([
            vehicle_prediction['translation'], 
            vehicle_prediction['rotation']
        ]),
        'translation': vehicle_prediction['translation'],
        'rotation': vehicle_prediction['rotation'],
        'category': original_category,
        'category_name': ['bottle', 'bowl', 'camera', 'can', 'laptop', 'mug'][original_category],
        'category_confidence': vehicle_prediction.get('confidence', 0.5),
        'size': np.array([1.0, 1.0, 1.0]),  # Default size
        'method': 'vehicle_to_original_conversion'
    }

# Model comparison utilities
def compare_model_architectures(original_model, vehicle_model, device):
    """
    Compare architectures between original HS-Pose and vehicle adaptation
    
    Args:
        original_model: Original HS-Pose model
        vehicle_model: Vehicle-adapted model
        device: Computation device
    
    Returns:
        dict: Architecture comparison results
    """
    # Count parameters
    original_params = sum(p.numel() for p in original_model.parameters())
    vehicle_params = sum(p.numel() for p in vehicle_model.parameters())
    
    # Test with sample inputs
    original_input = torch.randn(1, 3, 256, 256).to(device)
    vehicle_input = torch.randn(1, 4, 256, 256).to(device)
    
    # Measure inference times
    original_model.eval()
    vehicle_model.eval()
    
    with torch.no_grad():
        # Original model timing
        torch.cuda.synchronize()
        start_time = torch.cuda.Event(enable_timing=True)
        end_time = torch.cuda.Event(enable_timing=True)
        
        start_time.record()
        original_output = original_model(original_input)
        end_time.record()
        torch.cuda.synchronize()
        original_time = start_time.elapsed_time(end_time)
        
        # Vehicle model timing
        start_time.record()
        vehicle_output = vehicle_model(vehicle_input)
        end_time.record()
        torch.cuda.synchronize()
        vehicle_time = start_time.elapsed_time(end_time)
    
    comparison = {
        'original_params': original_params,
        'vehicle_params': vehicle_params,
        'param_difference': vehicle_params - original_params,
        'original_inference_time': original_time,
        'vehicle_inference_time': vehicle_time,
        'speed_ratio': original_time / vehicle_time,
        'original_outputs': list(original_output.keys()),
        'vehicle_outputs': list(vehicle_output.keys()),
        'shared_outputs': list(set(original_output.keys()) & set(vehicle_output.keys()))
    }
    
    return comparison

print("ð Original HS-Pose implementation loaded successfully!")
print("ð This module provides:")
print("  - OriginalHSPose: Complete original model with NOCS prediction")
print("  - OriginalHSPoseLoss: Original multi-task loss functions") 
print("  - Architecture comparison utilities")
print("  - Format conversion functions")
print("ð Use this for baseline comparisons with vehicle adaptations")
print("ðï¸ Based on official implementation from CVPR 2023 paper")


class OriginalHSPose(nn.Module):
    """
    Complete Original HS-Pose Model
    
    This is the full HS-Pose model as described in the original paper,
    including all prediction heads and post-processing functionality.
    """
    
    def __init__(self, input_channels=3, num_categories=6):
        """
        Initialize the complete HS-Pose model
        
        Args:
            input_channels (int): Number of input channels (3 for RGB)
            num_categories (int): Number of object categories
        """
        super(OriginalHSPose, self).__init__()
        
        self.num_categories = num_categories
        self.feature_extractor = OriginalHybridScopeFeatureExtractor(
            input_channels, num_categories
        )
        
        # Original category names from HS-Pose paper
        self.category_names = [
            'bottle', 'bowl', 'camera', 'can', 'laptop', 'mug'
        ]
        
    def forward(self, x):
        """Forward pass through the complete model"""
        return self.feature_extractor(x)
    
    def predict_pose(self, image, category_hint=None):
        """
        Predict pose using the original HS-Pose methodology
        
        Args:
            image (torch.Tensor): Input image tensor
            category_hint (str, optional): Category hint (not used in original)
        
        Returns:
            dict: Prediction results in original HS-Pose format
        """
        self.eval()
        with torch.no_grad():
            # Preprocess input
            if isinstance(image, np.ndarray):
                image = torch.from_numpy(image).float()
                if len(image.shape) == 3:
                    image = image.unsqueeze(0)
            
            # Ensure correct device
            device = next(self.parameters()).device
            image = image.to(device)
            
            # Forward pass
            outputs = self(image)
            
            # Extract predictions
            pose = outputs['pose'].squeeze()
            category_logits = outputs['category'].squeeze()
            shape = outputs['shape'].squeeze()
            size = outputs['size'].squeeze()
            
            # Get predicted category
            predicted_category = torch.argmax(category_logits).item()
            category_confidence = torch.softmax(category_logits, dim=0)[predicted_category].item()
            
            return {
                'pose': pose.cpu().numpy(),                    # 6D pose
                'translation': pose[:3].cpu().numpy(),        # 3D translation
                'rotation': pose[3:6].cpu().numpy(),          # 3D rotation
                'category': predicted_category,               # Category ID
                'category_name': self.category_names[predicted_category],
                'category_confidence': category_confidence,   # Classification confidence
                'shape': shape.cpu().numpy(),                 # Shape parameters
                'size': size.cpu().numpy(),                   # Size factor
                'method': 'original_hspose'                   # Method identifier
            }

# Loss functions for original HS-Pose
class OriginalHSPoseLoss(nn.Module):
    """
    Original HS-Pose loss function implementation
    
    Combines multiple loss components as described in the original paper:
    - Pose regression loss (translation + rotation)
    - Category classification loss
    - Shape regression loss
    - Size regression loss
    """
    
    def __init__(self, pose_weight=1.0, category_weight=0.5, shape_weight=0.3, size_weight=0.2):
        super(OriginalHSPoseLoss, self).__init__()
        
        self.pose_weight = pose_weight
        self.category_weight = category_weight
        self.shape_weight = shape_weight
        self.size_weight = size_weight
        
        # Loss functions
        self.pose_loss = nn.MSELoss()
        self.category_loss = nn.CrossEntropyLoss()
        self.shape_loss = nn.MSELoss()
        self.size_loss = nn.MSELoss()
    
    def forward(self, predictions, targets):
        """
        Compute multi-task loss for original HS-Pose
        
        Args:
            predictions (dict): Model predictions
            targets (dict): Ground truth targets
        
        Returns:
            dict: Individual and total losses
        """
        # Pose loss (translation + rotation)
        pose_loss = self.pose_loss(predictions['pose'], targets['pose'])
        
        # Category classification loss
        category_loss = self.category_loss(predictions['category'], targets['category'])
        
        # Shape regression loss
        shape_loss = self.shape_loss(predictions['shape'], targets['shape'])
        
        # Size regression loss
        size_loss = self.size_loss(predictions['size'], targets['size'])
        
        # Total weighted loss
        total_loss = (
            self.pose_weight * pose_loss +
            self.category_weight * category_loss +
            self.shape_weight * shape_loss +
            self.size_weight * size_loss
        )
        
        return {
            'total_loss': total_loss,
            'pose_loss': pose_loss,
            'category_loss': category_loss,
            'shape_loss': shape_loss,
            'size_loss': size_loss
        }

# Utility functions for original HS-Pose compatibility
def load_original_hspose_model(checkpoint_path, device='cpu'):
    """
    Load a trained original HS-Pose model
    
    Args:
        checkpoint_path (str): Path to model checkpoint
        device (str): Device to load model on
    
    Returns:
        OriginalHSPose: Loaded model
    """
    model = OriginalHSPose()
    
    if checkpoint_path and os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"Loaded original HS-Pose model from {checkpoint_path}")
    else:
        print("Initializing original HS-Pose with random weights")
    
    model.to(device)
    return model

def convert_vehicle_to_original_format(vehicle_prediction):
    """
    Convert vehicle pose prediction to original HS-Pose format
    
    This function helps compare results between our vehicle adaptation
    and the original HS-Pose implementation.
    
    Args:
        vehicle_prediction (dict): Vehicle pose prediction
    
    Returns:
        dict: Prediction in original HS-Pose format
    """
    # Map vehicle categories to original categories (for comparison)
    vehicle_to_original_category = {
        0: 2,  # car -> camera (similar size object)
        1: 4,  # motorcycle -> laptop (similar complexity)
        2: 0   # bicycle -> bottle (simple structure)
    }
    
    original_category = vehicle_to_original_category.get(
        vehicle_prediction.get('predicted_category', 0), 0
    )
    
    return {
        'pose': vehicle_prediction['translation'].tolist() + vehicle_prediction['rotation'].tolist(),
        'translation': vehicle_prediction['translation'],
        'rotation': vehicle_prediction['rotation'],
        'category': original_category,
        'category_name': ['bottle', 'bowl', 'camera', 'can', 'laptop', 'mug'][original_category],
        'category_confidence': vehicle_prediction.get('confidence', 0.5),
        'shape': [1.0, 1.0, 1.0],  # Default shape
        'size': 1.0,  # Default size
        'method': 'vehicle_to_original_conversion'
    }

# Model comparison utilities
def compare_model_outputs(original_model, vehicle_model, test_input_rgb, test_input_rgbd, device):
    """
    Compare outputs between original HS-Pose and vehicle adaptation
    
    Args:
        original_model: Original HS-Pose model
        vehicle_model: Vehicle-adapted model
        test_input_rgb: RGB test input for original model
        test_input_rgbd: RGB+Depth test input for vehicle model
        device: Computation device
    
    Returns:
        dict: Comparison results
    """
    original_model.eval()
    vehicle_model.eval()
    
    with torch.no_grad():
        # Original model prediction
        original_output = original_model(test_input_rgb)
        original_pred = original_model.predict_pose(test_input_rgb)
        
        # Vehicle model prediction
        vehicle_output = vehicle_model(test_input_rgbd)
        vehicle_pred = vehicle_model.predict_pose(test_input_rgbd)
        
        # Convert for comparison
        vehicle_as_original = convert_vehicle_to_original_format(vehicle_pred)
        
        comparison = {
            'original_prediction': original_pred,
            'vehicle_prediction': vehicle_pred,
            'vehicle_as_original': vehicle_as_original,
            'pose_difference': np.linalg.norm(
                np.array(original_pred['pose']) - np.array(vehicle_as_original['pose'])
            ),
            'feature_similarity': F.cosine_similarity(
                original_output['features'].flatten(),
                vehicle_output['features'].flatten(),
                dim=0
            ).item()
        }
    
    return comparison

print("ð Original HS-Pose implementation loaded successfully!")
print("ð This module provides:")
print("  - OriginalHSPose: Complete original model")
print("  - OriginalHSPoseLoss: Original loss functions") 
print("  - Comparison utilities for evaluation")
print("  - Format conversion functions")
print("ð Use this for baseline comparisons with vehicle adaptations")